import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, classification_report

from src.loader import load_customer_features

st.title("ğŸ¤– Machine Learning Models & Risk Analysis")

df = load_customer_features()

tab1, tab2, tab3 = st.tabs([
    "ğŸ”¹ Model 1 (Simple Linear Risk Score)",
    "ğŸ”¹ Model 2 (Nonlinear / Interaction Risk Score)",
    "ğŸ”¹ PCA & Feature Importance"
])

with tab1:
    st.header("ğŸ“Œ Model 1: Simple Risk Score â†’ Loan Approval Prediction")

    features = [
        'total_spent', 'avg_transaction', 'transaction_count', 'spending_std',
        'luxury', 'misc', 'necessity', 'wellbeing'
    ]
    df1 = df.copy()

    # normalize
    for col in features:
        r = df1[col].max() - df1[col].min()
        df1[col + "_norm"] = (df1[col] - df1[col].min()) / (r if r != 0 else 1)

    # simple linear risk
    df1['risk_score'] = (
        0.3 * df1['spending_std_norm'] +
        0.25 * df1['luxury_norm'] +
        0.10 * df1['misc_norm'] -
        0.20 * df1['necessity_norm'] -
        0.15 * df1['wellbeing_norm']
    )

    threshold = df1['risk_score'].median()
    df1['loan_approved'] = np.where(df1['risk_score'] < threshold, 1, 0)

    st.subheader("ğŸ“Š Risk Score Distribution")
    fig, ax = plt.subplots()
    sns.histplot(df1['risk_score'], kde=True, ax=ax)
    st.pyplot(fig)

    st.write(f"ğŸ“Œ **Approval Rate:** {df1['loan_approved'].mean().round(3)}")

    # model training
    X = df1[[c for c in df1.columns if c.endswith("_norm")]]
    y = df1['loan_approved']

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    logit = LogisticRegression(max_iter=2000, class_weight='balanced')
    logit.fit(X_train_scaled, y_train)
    p_log = logit.predict_proba(X_test_scaled)[:, 1]
    auc_log = roc_auc_score(y_test, p_log)

    st.write(f"### ğŸ”¹ Logistic Regression AUC: **{auc_log:.3f}**")
    st.text(classification_report(y_test, (p_log > 0.5).astype(int)))

    rf = RandomForestClassifier(n_estimators=300, class_weight='balanced', random_state=42)
    rf.fit(X_train, y_train)
    p_rf = rf.predict_proba(X_test)[:, 1]
    auc_rf = roc_auc_score(y_test, p_rf)

    st.write(f"### ğŸ”¹ Random Forest AUC: **{auc_rf:.3f}**")
    st.text(classification_report(y_test, (p_rf > 0.5).astype(int)))

    st.markdown("""
    ### ğŸ“ Interpretation
    - Model 1 learns extremely simple linear patterns â†’ ë„ˆë¬´ ì˜ í•™ìŠµí•¨ (AUC ~1.0)
    - ì‹¤ì œ ê³ ê° í–‰ë™ë³´ë‹¤ í›¨ì”¬ ë‹¨ìˆœí•œ êµ¬ì¡°ë¼ **ê³¼ì í•©ëœ pseudo-labelì„ ê·¸ëŒ€ë¡œ ì¬í˜„í•œ ê²ƒ**
    - ì¦‰, **í˜„ì‹¤ì„±ì€ ë¶€ì¡±í•˜ì§€ë§Œ, ìœ„í—˜ ì ìˆ˜ êµ¬ì¡°ë¥¼ ëª¨ë¸ì´ ê·¸ëŒ€ë¡œ ë”°ë¼ê°„ë‹¤ëŠ” ê²ƒ**ì„ ë³´ì—¬ì¤Œ
    """)

with tab2:
    st.header("ğŸ“Œ Model 2: Nonlinear + Interaction Features")

    df2 = df.copy()
    cols = ['total_spent','avg_transaction','transaction_count','spending_std',
            'luxury','misc','necessity','wellbeing']

    # normalize
    X0 = df2[cols].copy()
    for c in cols:
        r = X0[c].max() - X0[c].min()
        X0[c + "_n"] = (X0[c] - X0[c].min()) / (r if r != 0 else 1)

    Z = pd.DataFrame(index=df2.index)
    Z["std2"] = X0['spending_std_n'] ** 2
    Z["lux2"] = X0['luxury_n'] ** 2
    Z["wb2"] = X0['wellbeing_n'] ** 2
    Z["lux_std"] = X0['luxury_n'] * X0['spending_std_n']
    Z["nec_wb"] = X0['necessity_n'] * X0['wellbeing_n']
    Z["size_freq"] = X0['total_spent_n'] * X0['transaction_count_n']
    Z["ticket_mix"] = X0['avg_transaction_n'] * (X0['necessity_n'] - X0['luxury_n'])

    # nonlinear risk score
    risk = (
        0.35 * Z['std2'] +
        0.25 * Z['lux2'] +
        0.20 * Z['misc_std'] if 'misc_std' in Z else 0 +
        0.12 * Z['wb2'] +
        0.10 * Z['ticket_mix']
    )

    risk += np.random.normal(0, 0.03, len(risk))
    thr = np.median(risk)
    y = (risk < thr).astype(int)

    df2['risk_score2'] = risk
    df2['loan_approved2'] = y

    st.subheader("ğŸ“Š New Nonlinear Risk Score Distribution")
    fig, ax = plt.subplots()
    sns.histplot(risk, kde=True, ax=ax)
    st.pyplot(fig)

    # model training
    X = pd.concat([X0[[c for c in X0 if c.endswith("_n")]], Z], axis=1)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, stratify=y, random_state=42
    )

    scaler = StandardScaler(with_mean=False)
    Xs_tr = scaler.fit_transform(X_train)
    Xs_te = scaler.transform(X_test)

    # logistic
    logit = LogisticRegression(max_iter=2000, class_weight='balanced')
    logit.fit(Xs_tr, y_train)
    p1 = logit.predict_proba(Xs_te)[:, 1]
    auc1 = roc_auc_score(y_test, p1)

    st.write(f"### ğŸ”¹ Logistic Regression AUC: **{auc1:.3f}**")
    st.text(classification_report(y_test, (p1 > 0.5).astype(int)))

    # RF
    rf = RandomForestClassifier(n_estimators=400, class_weight='balanced', random_state=42)
    rf.fit(X_train, y_train)
    p2 = rf.predict_proba(X_test)[:, 1]
    auc2 = roc_auc_score(y_test, p2)

    st.write(f"### ğŸ”¹ Random Forest AUC: **{auc2:.3f}**")
    st.text(classification_report(y_test, (p2 > 0.5).astype(int)))

    st.markdown("""
    ### ğŸ“ Interpretation
    - More complex features â†’ ëª¨ë¸ì´ ì™„ë²½í•˜ê²Œ ì¬í˜„ ëª»í•¨ â†’ AUCê°€ 0.7ëŒ€
    - í˜„ì‹¤ì  ì†Œë¹„ íŒ¨í„´ì€ ë‹¨ìˆœ ì„ í˜• ê·œì¹™ë³´ë‹¤ í›¨ì”¬ ë³µì¡í•˜ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸
    - ì¦‰ **Model2ëŠ” í˜„ì‹¤ í–‰ë™ì„ ë” ë¹„ìŠ·í•˜ê²Œ ë°˜ì˜í•œ pseudo-label êµ¬ì¡°**
    """)
    
with tab3:
    st.header("ğŸ“Œ PCA & Feature Importance")

    cols = ['total_spent','avg_transaction','transaction_count','spending_std']
    X = df[cols].dropna()

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    from sklearn.cluster import KMeans
    from sklearn.decomposition import PCA

    kmeans = KMeans(n_clusters=3, random_state=42)
    labels = kmeans.fit_predict(X_scaled)

    pca = PCA(n_components=2)
    components = pca.fit_transform(X_scaled)

    st.subheader("ğŸ¨ PCA Projection with KMeans Clusters")
    fig, ax = plt.subplots(figsize=(7,5))
    scatter = ax.scatter(components[:,0], components[:,1], c=labels, cmap='viridis')
    plt.colorbar(scatter)
    st.pyplot(fig)

    st.subheader("ğŸ“Œ Cluster Group Statistics")
    df_cluster = df.copy()
    df_cluster['cluster'] = labels
    st.dataframe(df_cluster.groupby('cluster')[cols].mean())

    st.markdown("""
    ### Interpretation
    - **Cluster 0:** Heavy spenders + frequent transactions  
    - **Cluster 1:** Medium spending  
    - **Cluster 2:** Low-volume but high-variance users  
    """)
